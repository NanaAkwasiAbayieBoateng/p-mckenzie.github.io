---
# Posts need to have the `post` layout
layout: post

# The title of your post
title: "Reinforcement learning for 2048 - pt. 2"

# (Optional) Write a short (~150 characters) description of each blog post.
# This description is used to preview the page on search engines, social media, etc.
description: >
  A reinforcement learning approach to the game 2048, including manually coding the game and implementing a neural network to learn how to play. 
  The second of a two-part series (part 1 available [here](https://p-mckenzie.github.io/2019/10/11/2048-part-1/)), covering training the neural network.

# (Optional) Link to an image that represents your blog post.
# The aspect ratio should be ~16:9.
image: /assets/img/2048/header.png

# You can hide the description and/or image from the output
# (only visible to search engines) by setting:
hide_description: false
hide_image: false

categories: [predictive]
tags: []
languages: [Python]
---
![]({{site.url}}/assets/img/2048/header.png){:.lead}

<!--more-->

* dummy list
{:toc}

# Series recap
2048 is a game where the player chooses one of 4 actions, when presented with a board layout, to combine tiles until
they reach a value of 2048 (or larger). Part of a game would look like:

<img src="{{site.url}}/assets/img/2048/2048.gif" height="500">

My implementation of the game logic, using only numpy, is described in the previous [post]({{site.url}}/2019/10/11/2048-part-1/) 
in this 2-part series.

# Next steps
Our desired end result in this post is a neural network which can consistently achieve the 2048 tile, when given a new game. 
To achieve this, we will allow the network to repeatedly attempt different strategies, "learning" from each game's results
until its performance improves.

> Too often those writing about data science skip over describing the level of trial and error required in formulating 
a problem appropriately. 
> 
> In the name of transparency, I'm going to write about 
the evolution of my model formulation. The goal of this post is to detail the logic behind my choices and outline any 
lessons learned, rather than to present a fully-formed solution. 
>
> Frankly, I don't know that I will be able to build a network that is capable of achieving the 2048 tile consistently, 
but that doesn't mean this isn't a worthwhile exercise.

# Formulation
Let's generally represent a layout mathematically as a vector, $$L$$. 
This layout should be passed into a model, which outputs a probability distribution (let's call it $$\hat{Y}$$)
for the 4 moves (up, left, down, and right, respectively). Ignoring the linear algebra that happens inside the model, 
generally $$model(L)=\hat{Y}$$. 

Here's a toy example layout.
```
[ 0,16, 8, 0,
  2,32, 4, 0,
  0, 0, 8, 0,
  0, 0, 0, 0]
```
For this toy layout, maybe it would be $$model(L)=\hat{Y}=[.1,.2,.6,.1]$$.

## Choosing moves
We could simply choose the move with highest probability (for our toy layout, we'd swipe down).
However, if we introduce more randomness at this stage, the network 
will attempt a wider variety of moves given the same layout, hopefully discovering better strategies. 
To accomplish this, we'll draw a random variable from this probability distribution. Let's call this $$X : \hat{Y} \rightarrow M$$, where
$$M$$ is a vector indicating the move selected.

Continuing our toy example, say $$X([.1,.2,.6,.1])$$ outputs `[0,1,0,0]`, indicating that the 
computer chose to swipe left (the second highest probability option was drawn).

So now we know the layout ($$L$$), what distribution computer recommended ($$\hat{Y}$$), what action was taken ($$M$$), and eventually 
(given several more moves/decisions) we'll know the result of the game. 

## Finding $$Y$$
Typically, when training any machine learning model, you have a set of known, correct labels, commonly referred to as $$Y$$, and you
repeatedly train the model until $$\hat{Y}$$ gets sufficiently close to $$Y$$, using whatever loss function you've selected. 
In reinforcement learning, there are no correct labels. We don't know, given our toy layout, what the best move is, we only know
the results of the entire game. How do we train?

Moving away from math for a moment, **we want our neural network to perform more moves like those that resulted in "good" games, and
less moves like those that resulted in "bad" games**. 

I'll now define $$Y=f(M, game)$$, where $$f$$ maps the move that was 
chosen and the overall game performance to a probability distribution that, during training, should encourage $$\hat{Y}$$ to 
approach $$M$$ if the game was "good", and diverge from $$M$$ if the game was "bad".

I've chosen to define

$$Y = f(M, game) =
\begin{cases}
   M &\text{if } \text{\textquotedblleft}good\text{\textquotedblright} game \\
   (1-M)/3 &\text{if } \text{\textquotedblleft}bad\text{\textquotedblright} game
\end{cases}
$$

<br>
<img src="{{site.url}}/assets/img/2048/calm.gif" height="250">

Listen to Mr. Weasley. We're doing fine. Let's look at our toy example.

We had $$M=X(\hat{Y})=X([.1,.2,.6,.1])=[0,1,0,0]$$, which simply means the network decided to swipe left, given a probability distribution.

**If this was a "good" game**, we want to perform more moves like $$M$$, so $$Y=M=[0,1,0,0]$$. In English, this is the same
as saying "yes, always swipe left!"

**If this was a "bad" game**, we don't want to make a move like $$M$$, but we don't know what move would be better. So 
$$Y=(1-M)/3=[.33,0,.33,.33]$$. In English, this is the same as saying "Not left, but I don't know which direction to swipe instead..."

## Loss
Now we have $$Y$$, we need to tell the network how to adjust its weights with some objective function in mind, so that $$\hat{Y}$$
slowly approaches $$Y$$. 
I've decided to train to minimize Mean Absolute Error, which calculates the average distance between $$\hat{Y}$$ and $$Y$$
for a single move:

$$Loss(Y, \hat{Y})=\sum_{i=0}^4 |\hat{Y_i}-Y_i|/4$$

For this toy example $$\hat{Y}=[.1,.2,.6,.1]$$, if the game was "good" and $$Y=M=[0,1,0,0]$$, the network would calculate loss as:
$$\frac{|.1-0|+|.2-1|+|.6-0|+|.1-0|}{4} = 0.4$$. To decrease this, the network should adjust 
model weights to increase the probability of swiping left, and decrease the probability of swiping in any other direction,
given our layout $$L$$.

If the game was "bad" and $$Y=(1-M)/3=[.33,0,.33,.33]$$, our L1Loss would be:
$$\frac{|.1-.33|+|.2-0|+|.6-.33|+|.1-.33|}{4} = 0.2325$$. To decrease this, the network should adjust model weights to decrease
the second term (probability of swiping left), while increasing the others.

This loss can be is averaged across multiple moves, to train in batches rather than on individual data points.

## Weighting loss
With our current formulation, each move matters the same, regardless of how "good" or how "bad" the game it contributed to is,
and regardless of how "important" that individual move is to the game. This may not be ideal, and perhaps some manual intervention
by weighting the loss function would help the network train more efficiently.

For example, a game that achieves the same goal (perhaps a tile of value 2048) in fewer moves should be "better" than one
that achieves that goal in more moves, though both are "good" and would use $$Y=M$$. Alternatively, maybe a game that achieves
2048 with a higher score is better.

In order to control this first issue, I'll sometimes be adding a weighting at the game level. 
Going forward I'll refer to this as a game penalty $$j$$, and it should range [0,1]. I'll use different methods to calculate
$$j$$, but it will always be designed to capture shades of how "good" the game was. Every move in a single game 
will have the same $$j$$, but $$j$$ varies between games.

For the second issue, moves towards the end of
the game are probably more important than moves at the start (does which direction you initially swipe really have an impact
on the full game?). Alternatively, moves when the board is crowded (has many non-zero tiles) probably matter more than 
when the board is relatively clear.

In order to control this second issue I'll also sometimes add a move weight, $$k$$.
It will be designed to differentiate moves in the same game, ranging from [0,1] to designate how "important"
that individual move is to the overall game performance. Note that moves in a single game will have different $$k$$ values.

So my final *weighted* L1Loss is actually 
$$
\displaystyle \sum_{game}^{n} j * \sum_{move}^{m_n} k * Loss(Y, \hat{Y})
$$


# Initial code
Before I discuss the different formulations I attempted, we need some code - I've decided to use 
[pytorch](https://pytorch.org/) to train my neural network, and need to set it up.

```python
import numpy as np
import torch
from torch import optim, nn
```
To ensure repeatability, I set both the torch and numpy random seeds.
```python
np.random.seed(1)
torch.manual_seed(1)
```
Finally, here's the network:
```python
# initialize network
inputSize = 16; outputSize = 4; neuronCountJ = 200; neuronCountK = 100

model = nn.Sequential(nn.Linear(inputSize, neuronCountJ),
                       nn.ReLU(), 
                       nn.Linear(neuronCountJ, neuronCountK),
                       nn.ReLU(),
                       nn.Linear(neuronCountK, outputSize),
                       nn.Softmax(dim=1),
                     )
```
It accepts 16 inputs (one for each tile), and will output 4 probabilities (one for each direction swipe).
I arbitrarily selected a network with two hidden layers (layer J and layer K), with 200 and 100 neurons, respectively.

I've also chosen to use the Adam optimizer with a learning rate of 1e-3, at least initially. This will probably change with 
different formulations, depending on performance.
```python
opt = optim.Adam(model.parameters(), lr=1e-3)
```
## Loss
```python
loss = nn.L1Loss()
```
I'm using L1Loss from pytorch.nn, which calculates the Mean Absolute Error between $$\hat{Y}=[.1,.2,.6,.1]$$ 
and $$Y=[0,1,0,0]$$. 

For this toy example, the network would calculate loss as:
$$\frac{|.1-0|+|.2-1|+|.6-0|+|.1-0|}{4} = 0.4$$.

Our optimizer will now find directions to move the model parameters that will reduce this loss, by making $$\hat{Y}$$ approach $$Y$$. 
In this case, it will increase the second term (probability of swiping left), while decreasing the others.

**This loss function currently assumes that our example move was a "good" move, and we want the network to make 
more of them. What if this was a "bad" move?**

$$\hat{Y}$$ won't change, that comes from the model. All we can do is change $$Y$$. For now, I'll represent our changed $$Y$$ as
$$f(Y)$$, because we need some function that maps the overall game performance to a label. 

I'll try a few combinations, but one 
potential option is $$f(Y)=1-Y$$, so that $$f(Y)=[1,0,1,1]$$ and our L1Loss for this toy example would be:
$$\frac{|.1-1|+|.2-0|+|.6-1|+|.1-1|}{4} = 0.6$$.

Our optimizer will still find directions to move the model parameters that will reduce this loss, by making $$\hat{Y}$$ approach $$f(Y)$$. 
In this case, it will decrease the second term (probability of swiping left), while increasing the others.

## Inputs
```python
from helper import GameDriver
data = GameDriver()
data.run_games(10)
```
The `GameDriver` class I've written (which can be viewed in [helper.py](https://github.com/p-mckenzie/2048/blob/master/helper.py))
allows me to repeatedly run games. In this case, I just ran 10 games, and the results are stored in the `data` object. 

Looking at `data.layouts[0][0]`, we are accessing the first of 10 games, and the first layout of that game. An example could be
`[0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0]`. Shaping it like a user
might see it, we would see:
```
[[0, 0, 2, 0],
 [0, 0, 4, 0],
 [0, 0, 0, 0],
 [0, 0, 0, 0]]
```

**Neural networks prefer normalized inputs**, which is to say each input variable should be centered at zero, with a small standard deviation.
This is, of course, not required, but aids in the training process and in keeping the weight values in the network small. 
I can't exactly normalize, since I don't know the end distribution of my data. 

> This is because a "bad" game will hardly ever see a large tile (say, 1024), but as the network becomes better larger tiles will be more 
> prevalent. Aditionally, "bad" games might be crowded, with lots of tiles on the board at once, while "good" games might 
> more efficiently combine tiles, leading to a larger number of 0 tiles in any given layout. 
>
> All of this to say, I expect my inputs' distribution to shift during training.

Since I don't know the end distribution, I'm going to choose a method to transform that doesn't depend on knowing the distribution's mean.
I've chosen a variation of the [min-max scaler](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)), 
after log-transforming my data, to transform the tile values to inputs a neural network will be happier with. Instead of `[0, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]` 
as my input options, I'll log-transform 
(with an exception for 0, of course), getting the range `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]`. I'll now re-scale by the max (11, which
represents the 2048 tile). This gives me the range `[0., 0.09090909, 0.18181818, 0.27272727, 0.36363636, 0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182, 0.90909091, 1.]`.

I would still like `mean=0`, but realistically this will be difficult to achieve. As a proxy, I'm gonna subtract `0.25` from each value,
with the justification that the median tile at any given point will be around an `8`, which equates to `0.27` repeating given our transform
so far.

So, in python:
```python
def transform(layout):
	return np.log2(np.where(layout==0, 1, layout))/11-.25
```

**Example:**

Randomly choosing a layout from one of the games I just ran (which is now stored in `data.layouts`:
```
[ 0,  0,   0, 2,
  2, 16,   8, 4,
 16, 64, 128, 8,
  2,  2,  32,16]
 ```

The neural network will see:

```
[-0.25      , -0.25      , -0.25      , -0.15909091, 
 -0.15909091,  0.11363636,  0.02272727, -0.06818182,  
  0.11363636,  0.29545455,  0.38636364,  0.02272727, 
 -0.15909091, -0.15909091,  0.20454545,  0.11363636]
```

## Connecting to the network
We've now figured out how to format a layout. All we have to do now is feed that layout to the network,
and then return the probability distribution to the game, so that it takes action given the appropriate distribution.

Note the games we ran when simply calling `data.run_games(10)` were run completely randomly, without input from our neural network. 
At each layout, the game was given equal probability of choosing each direction.

We, instead, want to use the network's generated probability distribution, so the random variable (representing the move)
can be drawn from $$\hat{Y}$$.

```python
data.run_games(10,
    method=lambda layout:model(torch.tensor(np.log2(np.where(layout==0, 1, layout))/11, 
	                           dtype=torch.float).reshape(1,-1)).detach().numpy().flatten())
```

# Code
Complete code can be found in my 2048 git [repository](https://github.com/p-mckenzie/2048). 

This section of the project relies solely on numpy. Additional packages (such as widgets and matploblib)
may be required to run any notebooks in the repository, and the deep learning will be built using pytorch.